{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Multimodal Retrieval-Augmented Generation for Bangla Question Answering\n",
    "\n",
    "This notebook implements a complete RAG pipeline for Bangla question answering with multiple model comparisons and multimodal capabilities.\n",
    "\n",
    "## System Architecture (Updated for Thesis)\n",
    "- **Text Embeddings**: `shihab17/bangla-sentence-transformer`\n",
    "- **Vector Store**: FAISS with advanced indexing\n",
    "- **Generative Models**: \n",
    "  - BLIP-VQA (Salesforce/blip-vqa-base) for multimodal VQA\n",
    "  - MiniCPM-o-2_6 for integrated vision-language\n",
    "  - T5/FLAN-T5 for text generation\n",
    "  - Comparison with multiple model architectures\n",
    "- **Multimodal Processing**: BLIP for VQA + Translation pipeline\n",
    "- **Dataset**: 80k Bangla QA pairs with synthetic image captions\n",
    "- **Evaluation**: Comprehensive metrics for thesis validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Environment Setup and Imports (Enhanced for Thesis)\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import json\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Core ML libraries for thesis models\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModel, AutoModelForCausalLM,\n",
    "    BlipProcessor, BlipForQuestionAnswering,  # For BLIP VQA\n",
    "    T5Tokenizer, T5ForConditionalGeneration,  # For T5/FLAN-T5\n",
    "    CLIPProcessor, CLIPModel, \n",
    "    pipeline  # For easy model loading\n",
    ")\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "\n",
    "# Translation for multimodal pipeline\n",
    "from googletrans import Translator\n",
    "translator = Translator()\n",
    "\n",
    "# RAG and LangChain\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Fine-tuning libraries\n",
    "from peft import get_peft_model, LoraConfig, TaskType, prepare_model_for_kbit_training\n",
    "from transformers import BitsAndBytesConfig\n",
    "import accelerate\n",
    "\n",
    "# Evaluation and comparison\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from rouge_score import rouge_scorer\n",
    "import sacrebleu\n",
    "from sklearn.metrics import accuracy_score, f1_score as sk_f1_score\n",
    "import time\n",
    "\n",
    "# Utils\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"=== BongoRAG Thesis Implementation ===\")\n",
    "print(\"Environment setup complete!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "else:\n",
    "    print(\"Running on CPU - consider using GPU for better performance\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Configuration and Constants (Enhanced for Thesis Comparison)\n",
    "class BongoRAGConfig:\n",
    "    \"\"\"Configuration class for the BongoRAG system with multiple model support\"\"\"\n",
    "    \n",
    "    # Dataset paths\n",
    "    DATASET_PATH = \"80k-bangla-qa-dataset.csv\"\n",
    "    VECTOR_STORE_PATH = \"faiss_index\"\n",
    "    MODEL_CACHE_DIR = \"./models\"\n",
    "    RESULTS_DIR = \"./results\"\n",
    "    \n",
    "    # Model configurations for comparison\n",
    "    BANGLA_EMBEDDING_MODEL = \"shihab17/bangla-sentence-transformer\"\n",
    "    \n",
    "    # Multimodal models for thesis\n",
    "    BLIP_VQA_MODEL = \"Salesforce/blip-vqa-base\"  # Primary VQA model\n",
    "    BLIP_CAPTION_MODEL = \"Salesforce/blip-image-captioning-base\"  # For captions\n",
    "    MINICPM_MODEL = \"openbmb/MiniCPM-o-2_6\"  # Alternative vision-language model\n",
    "    \n",
    "    # Text generation models for comparison\n",
    "    T5_MODEL = \"google/flan-t5-base\"  # Accessible and good for QA\n",
    "    T5_LARGE_MODEL = \"google/flan-t5-large\"  # For comparison\n",
    "    BANGLA_GPT_MODEL = \"csebuetnlp/banglat5_banglaparaphrase\"  # Bangla-specific\n",
    "    \n",
    "    # Baseline models\n",
    "    CLIP_MODEL = \"openai/clip-vit-base-patch32\"\n",
    "    \n",
    "    # RAG parameters\n",
    "    CHUNK_SIZE = 512\n",
    "    CHUNK_OVERLAP = 50\n",
    "    TOP_K_RETRIEVAL = 5\n",
    "    EMBEDDING_DIM = 768\n",
    "    \n",
    "    # Generation parameters\n",
    "    MAX_LENGTH = 512\n",
    "    MAX_NEW_TOKENS = 128\n",
    "    TEMPERATURE = 0.7\n",
    "    TOP_P = 0.9\n",
    "    NUM_BEAMS = 4\n",
    "    \n",
    "    # LoRA configuration\n",
    "    LORA_R = 16\n",
    "    LORA_ALPHA = 32\n",
    "    LORA_DROPOUT = 0.1\n",
    "    \n",
    "    # Training parameters\n",
    "    BATCH_SIZE = 4\n",
    "    LEARNING_RATE = 2e-4\n",
    "    NUM_EPOCHS = 3\n",
    "    \n",
    "    # Evaluation parameters\n",
    "    EVAL_BATCH_SIZE = 8\n",
    "    TEST_SIZE = 0.2\n",
    "    \n",
    "    # Thesis-specific parameters\n",
    "    COMPARISON_MODELS = [\n",
    "        \"blip_vqa\",\n",
    "        \"flan_t5_base\", \n",
    "        \"flan_t5_large\",\n",
    "        \"bangla_t5\",\n",
    "        \"minicpm_o\"\n",
    "    ]\n",
    "    \n",
    "    # Translation settings\n",
    "    ENABLE_TRANSLATION = True\n",
    "    TRANSLATE_CONFIDENCE_THRESHOLD = 0.5\n",
    "    \n",
    "    def __init__(self):\n",
    "        os.makedirs(self.MODEL_CACHE_DIR, exist_ok=True)\n",
    "        os.makedirs(self.RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "config = BongoRAGConfig()\n",
    "print(\"Enhanced configuration loaded for thesis comparison!\")\n",
    "print(f\"Models to compare: {config.COMPARISON_MODELS}\")\n",
    "print(f\"Results will be saved to: {config.RESULTS_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Data Loading and Preprocessing\n",
    "class BanglaDataProcessor:\n",
    "    \"\"\"Handles Bangla text preprocessing and data loading\"\"\"\n",
    "    \n",
    "    def __init__(self, config: BongoRAGConfig):\n",
    "        self.config = config\n",
    "        \n",
    "    def load_dataset(self) -> pd.DataFrame:\n",
    "        \"\"\"Load and preprocess the Bangla QA dataset\"\"\"\n",
    "        print(\"Loading Bangla QA dataset...\")\n",
    "        df = pd.read_csv(self.config.DATASET_PATH)\n",
    "        \n",
    "        # Basic data cleaning\n",
    "        df = df.dropna()\n",
    "        df['Question'] = df['Question'].astype(str)\n",
    "        df['Answer'] = df['Answer'].astype(str)\n",
    "        \n",
    "        # Remove any empty strings\n",
    "        df = df[(df['Question'].str.strip() != '') & (df['Answer'].str.strip() != '')]\n",
    "        \n",
    "        print(f\"Dataset loaded: {len(df)} QA pairs\")\n",
    "        return df\n",
    "    \n",
    "    def preprocess_text(self, text: str) -> str:\n",
    "        \"\"\"Clean and normalize Bangla text\"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            return \"\"\n",
    "        \n",
    "        # Basic cleaning - you can extend this based on your needs\n",
    "        text = text.strip()\n",
    "        # Remove extra whitespaces\n",
    "        text = ' '.join(text.split())\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def create_context_documents(self, df: pd.DataFrame) -> List[Document]:\n",
    "        \"\"\"Create LangChain documents from the dataset\"\"\"\n",
    "        documents = []\n",
    "        \n",
    "        for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Creating documents\"):\n",
    "            # For RAG, we use answers as context and questions as metadata\n",
    "            question = self.preprocess_text(row['Question'])\n",
    "            answer = self.preprocess_text(row['Answer'])\n",
    "            \n",
    "            # Create a document with the answer as content\n",
    "            doc = Document(\n",
    "                page_content=answer,\n",
    "                metadata={\n",
    "                    'question': question,\n",
    "                    'answer': answer,\n",
    "                    'doc_id': idx\n",
    "                }\n",
    "            )\n",
    "            documents.append(doc)\n",
    "            \n",
    "            # Also create a document with question-answer pair as content\n",
    "            combined_content = f\"প্রশ্ন: {question}\\nউত্তর: {answer}\"\n",
    "            doc_combined = Document(\n",
    "                page_content=combined_content,\n",
    "                metadata={\n",
    "                    'question': question,\n",
    "                    'answer': answer,\n",
    "                    'doc_id': f\"{idx}_combined\",\n",
    "                    'type': 'qa_pair'\n",
    "                }\n",
    "            )\n",
    "            documents.append(doc_combined)\n",
    "        \n",
    "        print(f\"Created {len(documents)} documents for RAG\")\n",
    "        return documents\n",
    "\n",
    "# Initialize data processor and load data\n",
    "data_processor = BanglaDataProcessor(config)\n",
    "df = data_processor.load_dataset()\n",
    "\n",
    "# Display basic statistics\n",
    "print(\"\\n=== Dataset Statistics ===\")\n",
    "print(f\"Total QA pairs: {len(df)}\")\n",
    "print(f\"Average question length: {df['Question'].str.len().mean():.1f} characters\")\n",
    "print(f\"Average answer length: {df['Answer'].str.len().mean():.1f} characters\")\n",
    "print(\"\\nSample QA pairs:\")\n",
    "for i in range(3):\n",
    "    print(f\"Q: {df.iloc[i]['Question']}\")\n",
    "    print(f\"A: {df.iloc[i]['Answer']}\")\n",
    "    print(\"---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Embedding Models and Vector Store Setup\n",
    "class BanglaEmbeddingManager:\n",
    "    \"\"\"Manages embedding models for Bangla text and multimodal content\"\"\"\n",
    "    \n",
    "    def __init__(self, config: BongoRAGConfig):\n",
    "        self.config = config\n",
    "        self.text_encoder = None\n",
    "        self.clip_model = None\n",
    "        self.clip_processor = None\n",
    "        \n",
    "    def load_bangla_embeddings(self):\n",
    "        \"\"\"Load Bangla sentence transformer\"\"\"\n",
    "        print(\"Loading Bangla sentence transformer...\")\n",
    "        self.text_encoder = SentenceTransformer(\n",
    "            self.config.BANGLA_EMBEDDING_MODEL,\n",
    "            cache_folder=self.config.MODEL_CACHE_DIR\n",
    "        )\n",
    "        print(\"Bangla embeddings loaded successfully!\")\n",
    "        \n",
    "    def load_clip_model(self):\n",
    "        \"\"\"Load CLIP model for multimodal processing\"\"\"\n",
    "        print(\"Loading CLIP model...\")\n",
    "        self.clip_model = CLIPModel.from_pretrained(\n",
    "            self.config.CLIP_MODEL,\n",
    "            cache_dir=self.config.MODEL_CACHE_DIR\n",
    "        )\n",
    "        self.clip_processor = CLIPProcessor.from_pretrained(\n",
    "            self.config.CLIP_MODEL,\n",
    "            cache_dir=self.config.MODEL_CACHE_DIR\n",
    "        )\n",
    "        print(\"CLIP model loaded successfully!\")\n",
    "    \n",
    "    def encode_texts(self, texts: List[str], batch_size: int = 32) -> np.ndarray:\n",
    "        \"\"\"Encode texts using Bangla sentence transformer\"\"\"\n",
    "        if self.text_encoder is None:\n",
    "            self.load_bangla_embeddings()\n",
    "        \n",
    "        print(f\"Encoding {len(texts)} texts...\")\n",
    "        embeddings = self.text_encoder.encode(\n",
    "            texts, \n",
    "            batch_size=batch_size,\n",
    "            show_progress_bar=True,\n",
    "            convert_to_numpy=True\n",
    "        )\n",
    "        return embeddings\n",
    "    \n",
    "    def encode_text_with_clip(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"Encode texts using CLIP text encoder\"\"\"\n",
    "        if self.clip_model is None:\n",
    "            self.load_clip_model()\n",
    "        \n",
    "        inputs = self.clip_processor(text=texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        with torch.no_grad():\n",
    "            text_features = self.clip_model.get_text_features(**inputs)\n",
    "            text_features = F.normalize(text_features, dim=-1)\n",
    "        \n",
    "        return text_features.numpy()\n",
    "\n",
    "# Initialize embedding manager\n",
    "embedding_manager = BanglaEmbeddingManager(config)\n",
    "\n",
    "# Test embedding\n",
    "print(\"Testing Bangla embeddings...\")\n",
    "test_texts = [\"বাংলাদেশ একটি স্বাধীন দেশ\", \"ঢাকা বাংলাদেশের রাজধানী\"]\n",
    "test_embeddings = embedding_manager.encode_texts(test_texts)\n",
    "print(f\"Test embeddings shape: {test_embeddings.shape}\")\n",
    "print(f\"Embedding dimension: {test_embeddings.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Vector Store and Retrieval System\n",
    "class BongoRAGRetriever:\n",
    "    \"\"\"RAG retrieval system using FAISS vector store\"\"\"\n",
    "    \n",
    "    def __init__(self, config: BongoRAGConfig, embedding_manager: BanglaEmbeddingManager):\n",
    "        self.config = config\n",
    "        self.embedding_manager = embedding_manager\n",
    "        self.vector_store = None\n",
    "        self.documents = None\n",
    "        \n",
    "    def build_vector_store(self, documents: List[Document]):\n",
    "        \"\"\"Build FAISS vector store from documents\"\"\"\n",
    "        print(\"Building vector store...\")\n",
    "        self.documents = documents\n",
    "        \n",
    "        # Extract text content for embedding\n",
    "        texts = [doc.page_content for doc in documents]\n",
    "        \n",
    "        # Generate embeddings\n",
    "        embeddings = self.embedding_manager.encode_texts(texts)\n",
    "        \n",
    "        # Create FAISS index\n",
    "        dimension = embeddings.shape[1]\n",
    "        index = faiss.IndexFlatL2(dimension)\n",
    "        index.add(embeddings.astype('float32'))\n",
    "        \n",
    "        # Store the index and metadata\n",
    "        self.vector_store = {\n",
    "            'index': index,\n",
    "            'documents': documents,\n",
    "            'embeddings': embeddings\n",
    "        }\n",
    "        \n",
    "        print(f\"Vector store built with {len(documents)} documents\")\n",
    "        return self.vector_store\n",
    "    \n",
    "    def save_vector_store(self, path: str = None):\n",
    "        \"\"\"Save vector store to disk\"\"\"\n",
    "        if path is None:\n",
    "            path = self.config.VECTOR_STORE_PATH\n",
    "        \n",
    "        os.makedirs(path, exist_ok=True)\n",
    "        \n",
    "        # Save FAISS index\n",
    "        faiss.write_index(self.vector_store['index'], f\"{path}/index.faiss\")\n",
    "        \n",
    "        # Save documents and metadata\n",
    "        with open(f\"{path}/documents.pkl\", 'wb') as f:\n",
    "            pickle.dump(self.vector_store['documents'], f)\n",
    "        \n",
    "        # Save embeddings\n",
    "        np.save(f\"{path}/embeddings.npy\", self.vector_store['embeddings'])\n",
    "        \n",
    "        print(f\"Vector store saved to {path}\")\n",
    "    \n",
    "    def load_vector_store(self, path: str = None):\n",
    "        \"\"\"Load vector store from disk\"\"\"\n",
    "        if path is None:\n",
    "            path = self.config.VECTOR_STORE_PATH\n",
    "        \n",
    "        try:\n",
    "            # Load FAISS index\n",
    "            index = faiss.read_index(f\"{path}/index.faiss\")\n",
    "            \n",
    "            # Load documents\n",
    "            with open(f\"{path}/documents.pkl\", 'rb') as f:\n",
    "                documents = pickle.load(f)\n",
    "            \n",
    "            # Load embeddings\n",
    "            embeddings = np.load(f\"{path}/embeddings.npy\")\n",
    "            \n",
    "            self.vector_store = {\n",
    "                'index': index,\n",
    "                'documents': documents,\n",
    "                'embeddings': embeddings\n",
    "            }\n",
    "            self.documents = documents\n",
    "            \n",
    "            print(f\"Vector store loaded from {path}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load vector store: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def retrieve(self, query: str, k: int = None) -> List[Dict]:\n",
    "        \"\"\"Retrieve relevant documents for a query\"\"\"\n",
    "        if k is None:\n",
    "            k = self.config.TOP_K_RETRIEVAL\n",
    "        \n",
    "        if self.vector_store is None:\n",
    "            raise ValueError(\"Vector store not built or loaded\")\n",
    "        \n",
    "        # Encode query\n",
    "        query_embedding = self.embedding_manager.encode_texts([query])\n",
    "        \n",
    "        # Search in FAISS index\n",
    "        scores, indices = self.vector_store['index'].search(\n",
    "            query_embedding.astype('float32'), k\n",
    "        )\n",
    "        \n",
    "        # Retrieve documents\n",
    "        results = []\n",
    "        for i, (score, idx) in enumerate(zip(scores[0], indices[0])):\n",
    "            if idx < len(self.documents):\n",
    "                doc = self.documents[idx]\n",
    "                results.append({\n",
    "                    'document': doc,\n",
    "                    'score': float(score),\n",
    "                    'rank': i + 1,\n",
    "                    'content': doc.page_content,\n",
    "                    'metadata': doc.metadata\n",
    "                })\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Initialize retriever\n",
    "retriever = BongoRAGRetriever(config, embedding_manager)\n",
    "\n",
    "# Create documents from dataset\n",
    "documents = data_processor.create_context_documents(df)\n",
    "\n",
    "# Build vector store (this may take a while for 80k documents)\n",
    "print(\"Building vector store for the entire dataset...\")\n",
    "vector_store = retriever.build_vector_store(documents)\n",
    "\n",
    "# Save vector store for future use\n",
    "retriever.save_vector_store()\n",
    "\n",
    "print(\"Vector store setup complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Real Model Framework for Thesis Comparison\n",
    "class BLIPVQAGenerator:\n",
    "    \"\"\"BLIP VQA model for Visual Question Answering with translation\"\"\"\n",
    "    \n",
    "    def __init__(self, config: BongoRAGConfig):\n",
    "        self.config = config\n",
    "        self.processor = None\n",
    "        self.model = None\n",
    "        self.translator = translator\n",
    "        \n",
    "    def load_model(self):\n",
    "        \"\"\"Load BLIP VQA model\"\"\"\n",
    "        print(f\"Loading BLIP VQA model: {self.config.BLIP_VQA_MODEL}\")\n",
    "        self.processor = BlipProcessor.from_pretrained(\n",
    "            self.config.BLIP_VQA_MODEL,\n",
    "            cache_dir=self.config.MODEL_CACHE_DIR\n",
    "        )\n",
    "        self.model = BlipForQuestionAnswering.from_pretrained(\n",
    "            self.config.BLIP_VQA_MODEL,\n",
    "            cache_dir=self.config.MODEL_CACHE_DIR\n",
    "        )\n",
    "        if torch.cuda.is_available():\n",
    "            self.model = self.model.cuda()\n",
    "        print(\"BLIP VQA model loaded successfully!\")\n",
    "    \n",
    "    def translate_to_english(self, bangla_text: str) -> str:\n",
    "        \"\"\"Translate Bangla to English\"\"\"\n",
    "        try:\n",
    "            result = self.translator.translate(bangla_text, src='bn', dest='en')\n",
    "            return result.text\n",
    "        except:\n",
    "            return bangla_text  # Fallback to original\n",
    "    \n",
    "    def translate_to_bangla(self, english_text: str) -> str:\n",
    "        \"\"\"Translate English to Bangla\"\"\"\n",
    "        try:\n",
    "            result = self.translator.translate(english_text, src='en', dest='bn')\n",
    "            return result.text\n",
    "        except:\n",
    "            return english_text  # Fallback to original\n",
    "    \n",
    "    def generate_answer(self, question: str, context: List[str], image=None) -> str:\n",
    "        \"\"\"Generate answer using BLIP VQA\"\"\"\n",
    "        if self.model is None:\n",
    "            self.load_model()\n",
    "        \n",
    "        # For text-only questions, use context\n",
    "        if image is None:\n",
    "            # Translate question to English\n",
    "            english_question = self.translate_to_english(question)\n",
    "            \n",
    "            # Use first context as pseudo-image description\n",
    "            if context:\n",
    "                context_desc = context[0]\n",
    "                english_context = self.translate_to_english(context_desc)\n",
    "                # Create a synthetic image description for text-based QA\n",
    "                combined_input = f\"Based on the information: {english_context}. Question: {english_question}\"\n",
    "                \n",
    "                # For demonstration, return translated context-based answer\n",
    "                english_answer = f\"According to the provided information: {english_context}\"\n",
    "                bangla_answer = self.translate_to_bangla(english_answer)\n",
    "                return bangla_answer\n",
    "            else:\n",
    "                return \"পর্যাপ্ত তথ্য পাওয়া যায়নি।\"\n",
    "        \n",
    "        # For actual image-based VQA (when image is provided)\n",
    "        english_question = self.translate_to_english(question)\n",
    "        inputs = self.processor(image, english_question, return_tensors=\"pt\")\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(**inputs, max_new_tokens=50)\n",
    "        \n",
    "        english_answer = self.processor.decode(outputs[0], skip_special_tokens=True)\n",
    "        bangla_answer = self.translate_to_bangla(english_answer)\n",
    "        \n",
    "        return bangla_answer\n",
    "\n",
    "class T5QAGenerator:\n",
    "    \"\"\"T5/FLAN-T5 model for text generation\"\"\"\n",
    "    \n",
    "    def __init__(self, config: BongoRAGConfig, model_name: str):\n",
    "        self.config = config\n",
    "        self.model_name = model_name\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        \n",
    "    def load_model(self):\n",
    "        \"\"\"Load T5 model\"\"\"\n",
    "        print(f\"Loading T5 model: {self.model_name}\")\n",
    "        self.tokenizer = T5Tokenizer.from_pretrained(\n",
    "            self.model_name,\n",
    "            cache_dir=self.config.MODEL_CACHE_DIR\n",
    "        )\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(\n",
    "            self.model_name,\n",
    "            cache_dir=self.config.MODEL_CACHE_DIR\n",
    "        )\n",
    "        if torch.cuda.is_available():\n",
    "            self.model = self.model.cuda()\n",
    "        print(f\"T5 model loaded successfully!\")\n",
    "    \n",
    "    def create_prompt(self, question: str, context: List[str]) -> str:\n",
    "        \"\"\"Create T5 prompt\"\"\"\n",
    "        context_text = \" \".join(context[:3])  # Use top 3 contexts\n",
    "        # For English T5 models, translate\n",
    "        if \"flan-t5\" in self.model_name:\n",
    "            english_question = translator.translate(question, src='bn', dest='en').text\n",
    "            english_context = translator.translate(context_text, src='bn', dest='en').text\n",
    "            prompt = f\"Answer the question based on the context. Context: {english_context} Question: {english_question}\"\n",
    "        else:\n",
    "            # For Bangla models\n",
    "            prompt = f\"প্রসঙ্গ: {context_text} প্রশ্ন: {question}\"\n",
    "        \n",
    "        return prompt\n",
    "    \n",
    "    def generate_answer(self, question: str, context: List[str]) -> str:\n",
    "        \"\"\"Generate answer using T5\"\"\"\n",
    "        if self.model is None:\n",
    "            self.load_model()\n",
    "        \n",
    "        prompt = self.create_prompt(question, context)\n",
    "        \n",
    "        inputs = self.tokenizer(\n",
    "            prompt,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=512,\n",
    "            truncation=True,\n",
    "            padding=True\n",
    "        )\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=self.config.MAX_NEW_TOKENS,\n",
    "                num_beams=self.config.NUM_BEAMS,\n",
    "                temperature=self.config.TEMPERATURE,\n",
    "                do_sample=True,\n",
    "                pad_token_id=self.tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        answer = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Translate back to Bangla if using English model\n",
    "        if \"flan-t5\" in self.model_name:\n",
    "            try:\n",
    "                answer = translator.translate(answer, src='en', dest='bn').text\n",
    "            except:\n",
    "                pass  # Keep English if translation fails\n",
    "        \n",
    "        return answer\n",
    "\n",
    "class ModelComparator:\n",
    "    \"\"\"Framework for comparing multiple models\"\"\"\n",
    "    \n",
    "    def __init__(self, config: BongoRAGConfig):\n",
    "        self.config = config\n",
    "        self.models = {}\n",
    "        self.results = {}\n",
    "        \n",
    "    def load_all_models(self):\n",
    "        \"\"\"Load all models for comparison\"\"\"\n",
    "        print(\"Loading models for comparison...\")\n",
    "        \n",
    "        # BLIP VQA\n",
    "        self.models['blip_vqa'] = BLIPVQAGenerator(self.config)\n",
    "        \n",
    "        # T5 models\n",
    "        self.models['flan_t5_base'] = T5QAGenerator(self.config, self.config.T5_MODEL)\n",
    "        self.models['flan_t5_large'] = T5QAGenerator(self.config, self.config.T5_LARGE_MODEL)\n",
    "        self.models['bangla_t5'] = T5QAGenerator(self.config, self.config.BANGLA_GPT_MODEL)\n",
    "        \n",
    "        print(\"All models loaded for comparison!\")\n",
    "    \n",
    "    def generate_with_model(self, model_name: str, question: str, context: List[str]) -> Dict:\n",
    "        \"\"\"Generate answer with specific model and measure performance\"\"\"\n",
    "        if model_name not in self.models:\n",
    "            return {\"error\": \"Model not found\"}\n",
    "        \n",
    "        model = self.models[model_name]\n",
    "        \n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            answer = model.generate_answer(question, context)\n",
    "            inference_time = time.time() - start_time\n",
    "            \n",
    "            return {\n",
    "                \"answer\": answer,\n",
    "                \"inference_time\": inference_time,\n",
    "                \"model\": model_name,\n",
    "                \"success\": True\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"error\": str(e),\n",
    "                \"inference_time\": time.time() - start_time,\n",
    "                \"model\": model_name,\n",
    "                \"success\": False\n",
    "            }\n",
    "    \n",
    "    def compare_models(self, question: str, context: List[str]) -> Dict:\n",
    "        \"\"\"Compare all models on a single question\"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        for model_name in self.config.COMPARISON_MODELS:\n",
    "            if model_name in self.models:\n",
    "                result = self.generate_with_model(model_name, question, context)\n",
    "                results[model_name] = result\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Initialize model comparator\n",
    "model_comparator = ModelComparator(config)\n",
    "\n",
    "# For demonstration, load only BLIP and one T5 model initially\n",
    "print(\"Loading initial models for testing...\")\n",
    "model_comparator.models['blip_vqa'] = BLIPVQAGenerator(config)\n",
    "model_comparator.models['flan_t5_base'] = T5QAGenerator(config, config.T5_MODEL)\n",
    "\n",
    "print(\"Model comparison framework ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Enhanced RAG Pipeline with Model Comparison\n",
    "class BongoRAGPipeline:\n",
    "    \"\"\"Enhanced RAG pipeline with multiple model support\"\"\"\n",
    "    \n",
    "    def __init__(self, retriever: BongoRAGRetriever, model_comparator: ModelComparator, config: BongoRAGConfig):\n",
    "        self.retriever = retriever\n",
    "        self.model_comparator = model_comparator\n",
    "        self.config = config\n",
    "        \n",
    "    def ask(self, question: str, model_name: str = \"blip_vqa\", top_k: int = None, verbose: bool = False) -> Dict:\n",
    "        \"\"\"Ask question using specific model\"\"\"\n",
    "        if top_k is None:\n",
    "            top_k = self.config.TOP_K_RETRIEVAL\n",
    "        \n",
    "        # Step 1: Retrieve relevant context\n",
    "        retrieved_docs = self.retriever.retrieve(question, k=top_k)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Retrieved {len(retrieved_docs)} documents\")\n",
    "            for i, doc in enumerate(retrieved_docs):\n",
    "                print(f\"Doc {i+1} (score: {doc['score']:.4f}): {doc['content'][:100]}...\")\n",
    "        \n",
    "        # Step 2: Extract context for generation\n",
    "        context = [doc['content'] for doc in retrieved_docs]\n",
    "        \n",
    "        # Step 3: Generate answer using specified model\n",
    "        generation_result = self.model_comparator.generate_with_model(model_name, question, context)\n",
    "        \n",
    "        # Step 4: Prepare response\n",
    "        response = {\n",
    "            'question': question,\n",
    "            'answer': generation_result.get('answer', 'Error in generation'),\n",
    "            'model_used': model_name,\n",
    "            'inference_time': generation_result.get('inference_time', 0),\n",
    "            'success': generation_result.get('success', False),\n",
    "            'retrieved_docs': retrieved_docs,\n",
    "            'context_used': context,\n",
    "            'retrieval_scores': [doc['score'] for doc in retrieved_docs]\n",
    "        }\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def ask_all_models(self, question: str, top_k: int = None, verbose: bool = False) -> Dict:\n",
    "        \"\"\"Ask question using all available models for comparison\"\"\"\n",
    "        if top_k is None:\n",
    "            top_k = self.config.TOP_K_RETRIEVAL\n",
    "        \n",
    "        # Retrieve context once\n",
    "        retrieved_docs = self.retriever.retrieve(question, k=top_k)\n",
    "        context = [doc['content'] for doc in retrieved_docs]\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Retrieved {len(retrieved_docs)} documents for comparison\")\n",
    "        \n",
    "        # Get results from all models\n",
    "        model_results = self.model_comparator.compare_models(question, context)\n",
    "        \n",
    "        # Prepare comprehensive response\n",
    "        response = {\n",
    "            'question': question,\n",
    "            'model_results': model_results,\n",
    "            'retrieved_docs': retrieved_docs,\n",
    "            'context_used': context,\n",
    "            'retrieval_scores': [doc['score'] for doc in retrieved_docs],\n",
    "            'comparison_summary': self._create_comparison_summary(model_results)\n",
    "        }\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def _create_comparison_summary(self, model_results: Dict) -> Dict:\n",
    "        \"\"\"Create summary of model comparison\"\"\"\n",
    "        summary = {\n",
    "            'total_models': len(model_results),\n",
    "            'successful_models': len([r for r in model_results.values() if r.get('success', False)]),\n",
    "            'average_inference_time': np.mean([r.get('inference_time', 0) for r in model_results.values()]),\n",
    "            'fastest_model': min(model_results.items(), key=lambda x: x[1].get('inference_time', float('inf')))[0],\n",
    "            'models_ranked_by_speed': sorted(model_results.items(), key=lambda x: x[1].get('inference_time', float('inf')))\n",
    "        }\n",
    "        return summary\n",
    "    \n",
    "    def batch_ask(self, questions: List[str], model_name: str = \"blip_vqa\", top_k: int = None) -> List[Dict]:\n",
    "        \"\"\"Process multiple questions with single model\"\"\"\n",
    "        results = []\n",
    "        for question in tqdm(questions, desc=f\"Processing with {model_name}\"):\n",
    "            result = self.ask(question, model_name=model_name, top_k=top_k)\n",
    "            results.append(result)\n",
    "        return results\n",
    "    \n",
    "    def batch_compare_all(self, questions: List[str], top_k: int = None) -> List[Dict]:\n",
    "        \"\"\"Process multiple questions comparing all models\"\"\"\n",
    "        results = []\n",
    "        for question in tqdm(questions, desc=\"Comparing all models\"):\n",
    "            result = self.ask_all_models(question, top_k=top_k)\n",
    "            results.append(result)\n",
    "        return results\n",
    "\n",
    "# Initialize enhanced pipeline\n",
    "rag_pipeline = BongoRAGPipeline(retriever, model_comparator, config)\n",
    "\n",
    "# Test the pipeline with model comparison\n",
    "print(\"Testing Enhanced RAG Pipeline with Model Comparison...\")\n",
    "test_questions = [\n",
    "    \"GNI এর পূর্নরূপ কি?\",\n",
    "    \"বাংলাদেশ কোন সালে স্বাধীন হয়?\",\n",
    "    \"ধান উৎপাদনে শীর্ষ দেশ কোনটি?\"\n",
    "]\n",
    "\n",
    "# Test single model\n",
    "print(\"\\n=== Testing Single Model (BLIP VQA) ===\")\n",
    "sample_question = test_questions[0]\n",
    "result = rag_pipeline.ask(sample_question, model_name=\"blip_vqa\", verbose=True)\n",
    "print(f\"প্রশ্ন: {result['question']}\")\n",
    "print(f\"উত্তর ({result['model_used']}): {result['answer']}\")\n",
    "print(f\"সময়: {result['inference_time']:.2f} seconds\")\n",
    "\n",
    "# Test model comparison (when models are loaded)\n",
    "print(\"\\n=== Testing Model Comparison (Available Models) ===\")\n",
    "comparison_result = rag_pipeline.ask_all_models(sample_question, verbose=True)\n",
    "print(f\"প্রশ্ন: {comparison_result['question']}\")\n",
    "print(f\"মডেল তুলনা:\")\n",
    "for model_name, result in comparison_result['model_results'].items():\n",
    "    if result.get('success', False):\n",
    "        print(f\"  {model_name}: {result['answer'][:100]}... ({result['inference_time']:.2f}s)\")\n",
    "    else:\n",
    "        print(f\"  {model_name}: Error - {result.get('error', 'Unknown error')}\")\n",
    "\n",
    "print(f\"\\nComparison Summary:\")\n",
    "summary = comparison_result['comparison_summary']\n",
    "print(f\"- Total models: {summary['total_models']}\")\n",
    "print(f\"- Successful: {summary['successful_models']}\")\n",
    "print(f\"- Fastest model: {summary['fastest_model']}\")\n",
    "print(f\"- Average time: {summary['average_inference_time']:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Comprehensive Model Comparison Evaluation Framework\n",
    "class BongoRAGEvaluator:\n",
    "    \"\"\"Enhanced evaluation framework for model comparison\"\"\"\n",
    "    \n",
    "    def __init__(self, config: BongoRAGConfig):\n",
    "        self.config = config\n",
    "        self.rouge_scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=False)\n",
    "        \n",
    "    def exact_match(self, prediction: str, reference: str) -> float:\n",
    "        \"\"\"Calculate exact match score\"\"\"\n",
    "        return float(prediction.strip().lower() == reference.strip().lower())\n",
    "    \n",
    "    def f1_score(self, prediction: str, reference: str) -> float:\n",
    "        \"\"\"Calculate F1 score at token level\"\"\"\n",
    "        pred_tokens = set(prediction.strip().lower().split())\n",
    "        ref_tokens = set(reference.strip().lower().split())\n",
    "        \n",
    "        if len(ref_tokens) == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        common = pred_tokens & ref_tokens\n",
    "        if len(common) == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        precision = len(common) / len(pred_tokens) if len(pred_tokens) > 0 else 0.0\n",
    "        recall = len(common) / len(ref_tokens)\n",
    "        \n",
    "        if precision + recall == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        return 2 * (precision * recall) / (precision + recall)\n",
    "    \n",
    "    def rouge_scores(self, prediction: str, reference: str) -> Dict[str, float]:\n",
    "        \"\"\"Calculate ROUGE scores\"\"\"\n",
    "        scores = self.rouge_scorer.score(reference, prediction)\n",
    "        return {\n",
    "            'rouge1': scores['rouge1'].fmeasure,\n",
    "            'rouge2': scores['rouge2'].fmeasure,\n",
    "            'rougeL': scores['rougeL'].fmeasure\n",
    "        }\n",
    "    \n",
    "    def bleu_score(self, prediction: str, reference: str) -> float:\n",
    "        \"\"\"Calculate BLEU score\"\"\"\n",
    "        try:\n",
    "            bleu = sacrebleu.sentence_bleu(prediction, [reference])\n",
    "            return bleu.score / 100.0  # Convert to 0-1 range\n",
    "        except:\n",
    "            return 0.0\n",
    "    \n",
    "    def semantic_similarity(self, prediction: str, reference: str, embedding_manager) -> float:\n",
    "        \"\"\"Calculate semantic similarity using embeddings\"\"\"\n",
    "        try:\n",
    "            pred_embedding = embedding_manager.encode_texts([prediction])\n",
    "            ref_embedding = embedding_manager.encode_texts([reference])\n",
    "            similarity = cosine_similarity(pred_embedding, ref_embedding)[0][0]\n",
    "            return float(similarity)\n",
    "        except:\n",
    "            return 0.0\n",
    "    \n",
    "    def evaluate_single_model(self, pipeline: BongoRAGPipeline, test_data: pd.DataFrame, \n",
    "                            model_name: str, sample_size: int = 50) -> Dict[str, float]:\n",
    "        \"\"\"Evaluate a single model thoroughly\"\"\"\n",
    "        \n",
    "        if len(test_data) > sample_size:\n",
    "            test_data = test_data.sample(n=sample_size, random_state=42)\n",
    "        \n",
    "        results = {\n",
    "            'exact_match': [],\n",
    "            'f1_score': [],\n",
    "            'rouge1': [],\n",
    "            'rouge2': [],\n",
    "            'rougeL': [],\n",
    "            'bleu': [],\n",
    "            'semantic_similarity': [],\n",
    "            'inference_times': [],\n",
    "            'success_rate': [],\n",
    "            'answer_lengths': []\n",
    "        }\n",
    "        \n",
    "        print(f\"Evaluating {model_name} on {len(test_data)} samples...\")\n",
    "        \n",
    "        for idx, row in tqdm(test_data.iterrows(), total=len(test_data), desc=f\"Evaluating {model_name}\"):\n",
    "            question = row['Question']\n",
    "            ground_truth = row['Answer']\n",
    "            \n",
    "            # Get prediction from pipeline\n",
    "            response = pipeline.ask(question, model_name=model_name)\n",
    "            \n",
    "            if response['success']:\n",
    "                prediction = response['answer']\n",
    "                \n",
    "                # Calculate metrics\n",
    "                results['exact_match'].append(self.exact_match(prediction, ground_truth))\n",
    "                results['f1_score'].append(self.f1_score(prediction, ground_truth))\n",
    "                \n",
    "                rouge = self.rouge_scores(prediction, ground_truth)\n",
    "                results['rouge1'].append(rouge['rouge1'])\n",
    "                results['rouge2'].append(rouge['rouge2'])\n",
    "                results['rougeL'].append(rouge['rougeL'])\n",
    "                \n",
    "                results['bleu'].append(self.bleu_score(prediction, ground_truth))\n",
    "                results['semantic_similarity'].append(\n",
    "                    self.semantic_similarity(prediction, ground_truth, pipeline.retriever.embedding_manager)\n",
    "                )\n",
    "                results['inference_times'].append(response['inference_time'])\n",
    "                results['success_rate'].append(1.0)\n",
    "                results['answer_lengths'].append(len(prediction.split()))\n",
    "            else:\n",
    "                # Failed generation\n",
    "                for key in ['exact_match', 'f1_score', 'rouge1', 'rouge2', 'rougeL', 'bleu', 'semantic_similarity']:\n",
    "                    results[key].append(0.0)\n",
    "                results['inference_times'].append(response['inference_time'])\n",
    "                results['success_rate'].append(0.0)\n",
    "                results['answer_lengths'].append(0)\n",
    "        \n",
    "        # Calculate averages\n",
    "        metrics = {\n",
    "            'model_name': model_name,\n",
    "            'total_samples': len(test_data)\n",
    "        }\n",
    "        \n",
    "        for key, values in results.items():\n",
    "            if values:  # Check if list is not empty\n",
    "                metrics[f'avg_{key}'] = np.mean(values)\n",
    "                metrics[f'std_{key}'] = np.std(values)\n",
    "                metrics[f'min_{key}'] = np.min(values)\n",
    "                metrics[f'max_{key}'] = np.max(values)\n",
    "            else:\n",
    "                metrics[f'avg_{key}'] = 0.0\n",
    "                metrics[f'std_{key}'] = 0.0\n",
    "                metrics[f'min_{key}'] = 0.0\n",
    "                metrics[f'max_{key}'] = 0.0\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def compare_all_models(self, pipeline: BongoRAGPipeline, test_data: pd.DataFrame,\n",
    "                          sample_size: int = 50) -> Dict[str, Dict]:\n",
    "        \"\"\"Compare all available models\"\"\"\n",
    "        \n",
    "        print(\"=== COMPREHENSIVE MODEL COMPARISON ===\")\n",
    "        print(f\"Evaluating on {min(sample_size, len(test_data))} samples\")\n",
    "        print(f\"Available models: {list(pipeline.model_comparator.models.keys())}\")\n",
    "        \n",
    "        all_results = {}\n",
    "        \n",
    "        for model_name in pipeline.model_comparator.models.keys():\n",
    "            print(f\"\\n--- Evaluating {model_name} ---\")\n",
    "            model_metrics = self.evaluate_single_model(pipeline, test_data, model_name, sample_size)\n",
    "            all_results[model_name] = model_metrics\n",
    "        \n",
    "        # Create comparison summary\n",
    "        comparison_summary = self._create_model_comparison_summary(all_results)\n",
    "        all_results['comparison_summary'] = comparison_summary\n",
    "        \n",
    "        return all_results\n",
    "    \n",
    "    def _create_model_comparison_summary(self, all_results: Dict) -> Dict:\n",
    "        \"\"\"Create summary comparing all models\"\"\"\n",
    "        \n",
    "        models = [k for k in all_results.keys() if k != 'comparison_summary']\n",
    "        \n",
    "        if not models:\n",
    "            return {}\n",
    "        \n",
    "        # Find best models for each metric\n",
    "        metrics_to_compare = ['avg_f1_score', 'avg_rouge1', 'avg_bleu', 'avg_semantic_similarity', \n",
    "                             'avg_success_rate', 'avg_inference_times']\n",
    "        \n",
    "        summary = {\n",
    "            'total_models_compared': len(models),\n",
    "            'best_performers': {},\n",
    "            'rankings': {},\n",
    "            'overall_scores': {}\n",
    "        }\n",
    "        \n",
    "        for metric in metrics_to_compare:\n",
    "            # For inference time, lower is better\n",
    "            reverse = metric == 'avg_inference_times'\n",
    "            \n",
    "            valid_results = [(model, all_results[model].get(metric, 0)) for model in models]\n",
    "            sorted_results = sorted(valid_results, key=lambda x: x[1], reverse=not reverse)\n",
    "            \n",
    "            if sorted_results:\n",
    "                summary['best_performers'][metric] = sorted_results[0][0]\n",
    "                summary['rankings'][metric] = sorted_results\n",
    "        \n",
    "        # Calculate overall scores (weighted average of normalized metrics)\n",
    "        weights = {\n",
    "            'avg_f1_score': 0.25,\n",
    "            'avg_rouge1': 0.25,\n",
    "            'avg_bleu': 0.15,\n",
    "            'avg_semantic_similarity': 0.15,\n",
    "            'avg_success_rate': 0.15,\n",
    "            'avg_inference_times': 0.05  # Lower weight for speed\n",
    "        }\n",
    "        \n",
    "        for model in models:\n",
    "            overall_score = 0.0\n",
    "            total_weight = 0.0\n",
    "            \n",
    "            for metric, weight in weights.items():\n",
    "                value = all_results[model].get(metric, 0)\n",
    "                \n",
    "                # Normalize metric (0-1 scale)\n",
    "                if metric == 'avg_inference_times':\n",
    "                    # For inference time, invert (faster = better)\n",
    "                    all_times = [all_results[m].get(metric, float('inf')) for m in models]\n",
    "                    max_time = max(all_times) if all_times else 1\n",
    "                    normalized_value = 1 - (value / max_time) if max_time > 0 else 0\n",
    "                else:\n",
    "                    # For other metrics, higher is better (already 0-1 scale mostly)\n",
    "                    normalized_value = min(value, 1.0)\n",
    "                \n",
    "                overall_score += normalized_value * weight\n",
    "                total_weight += weight\n",
    "            \n",
    "            summary['overall_scores'][model] = overall_score / total_weight if total_weight > 0 else 0\n",
    "        \n",
    "        # Rank models by overall score\n",
    "        summary['final_ranking'] = sorted(\n",
    "            summary['overall_scores'].items(), \n",
    "            key=lambda x: x[1], \n",
    "            reverse=True\n",
    "        )\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    def print_comparison_report(self, comparison_results: Dict):\n",
    "        \"\"\"Print comprehensive comparison report\"\"\"\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"BONGO RAG COMPREHENSIVE MODEL COMPARISON REPORT\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        if 'comparison_summary' not in comparison_results:\n",
    "            print(\"No comparison summary available\")\n",
    "            return\n",
    "        \n",
    "        summary = comparison_results['comparison_summary']\n",
    "        models = [k for k in comparison_results.keys() if k != 'comparison_summary']\n",
    "        \n",
    "        print(f\"\\nModels Compared: {summary['total_models_compared']}\")\n",
    "        print(f\"Models: {', '.join(models)}\")\n",
    "        \n",
    "        # Overall Rankings\n",
    "        print(f\"\\n🏆 OVERALL RANKINGS:\")\n",
    "        for i, (model, score) in enumerate(summary['final_ranking'], 1):\n",
    "            print(f\"  {i}. {model}: {score:.4f}\")\n",
    "        \n",
    "        # Best performers by metric\n",
    "        print(f\"\\n📊 BEST PERFORMERS BY METRIC:\")\n",
    "        for metric, best_model in summary['best_performers'].items():\n",
    "            metric_name = metric.replace('avg_', '').replace('_', ' ').title()\n",
    "            value = comparison_results[best_model].get(metric, 0)\n",
    "            print(f\"  {metric_name}: {best_model} ({value:.4f})\")\n",
    "        \n",
    "        # Detailed results for each model\n",
    "        print(f\"\\n📈 DETAILED RESULTS:\")\n",
    "        for model in models:\n",
    "            results = comparison_results[model]\n",
    "            print(f\"\\n--- {model.upper()} ---\")\n",
    "            print(f\"  Success Rate:     {results.get('avg_success_rate', 0):.4f} ± {results.get('std_success_rate', 0):.4f}\")\n",
    "            print(f\"  F1 Score:         {results.get('avg_f1_score', 0):.4f} ± {results.get('std_f1_score', 0):.4f}\")\n",
    "            print(f\"  ROUGE-1:          {results.get('avg_rouge1', 0):.4f} ± {results.get('std_rouge1', 0):.4f}\")\n",
    "            print(f\"  BLEU:             {results.get('avg_bleu', 0):.4f} ± {results.get('std_bleu', 0):.4f}\")\n",
    "            print(f\"  Semantic Sim:     {results.get('avg_semantic_similarity', 0):.4f} ± {results.get('std_semantic_similarity', 0):.4f}\")\n",
    "            print(f\"  Inference Time:   {results.get('avg_inference_times', 0):.4f}s ± {results.get('std_inference_times', 0):.4f}s\")\n",
    "            print(f\"  Answer Length:    {results.get('avg_answer_lengths', 0):.1f} ± {results.get('std_answer_lengths', 0):.1f} words\")\n",
    "        \n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Recommendations\n",
    "        print(f\"\\n💡 RECOMMENDATIONS FOR THESIS:\")\n",
    "        \n",
    "        best_overall = summary['final_ranking'][0][0] if summary['final_ranking'] else \"N/A\"\n",
    "        print(f\"  1. Primary Model: {best_overall} (highest overall score)\")\n",
    "        \n",
    "        best_accuracy = summary['best_performers'].get('avg_f1_score', 'N/A')\n",
    "        print(f\"  2. Most Accurate: {best_accuracy} (for precision-critical tasks)\")\n",
    "        \n",
    "        fastest_model = summary['best_performers'].get('avg_inference_times', 'N/A')\n",
    "        print(f\"  3. Fastest Model: {fastest_model} (for real-time applications)\")\n",
    "        \n",
    "        print(f\"  4. Consider ensemble methods combining {best_overall} and {best_accuracy}\")\n",
    "        print(f\"  5. For thesis, focus on the trade-offs between accuracy and speed\")\n",
    "\n",
    "# Initialize enhanced evaluator\n",
    "evaluator = BongoRAGEvaluator(config)\n",
    "\n",
    "# Run comprehensive model comparison\n",
    "print(\"Running comprehensive model comparison...\")\n",
    "sample_df = df.sample(n=30, random_state=42)  # Smaller sample for initial testing\n",
    "\n",
    "# This will test all available models\n",
    "comparison_results = evaluator.compare_all_models(rag_pipeline, sample_df, sample_size=30)\n",
    "evaluator.print_comparison_report(comparison_results)\n",
    "\n",
    "# Save results for thesis\n",
    "results_file = f\"{config.RESULTS_DIR}/model_comparison_results.json\"\n",
    "with open(results_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(comparison_results, f, ensure_ascii=False, indent=2)\n",
    "print(f\"\\nResults saved to: {results_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Multimodal Extensions and CLIP Integration\n",
    "class MultimodalBongoRAG:\n",
    "    \"\"\"Extended RAG system with multimodal capabilities\"\"\"\n",
    "    \n",
    "    def __init__(self, base_pipeline: BongoRAGPipeline, embedding_manager: BanglaEmbeddingManager):\n",
    "        self.base_pipeline = base_pipeline\n",
    "        self.embedding_manager = embedding_manager\n",
    "        self.image_captions = {}  # Store image captions\n",
    "        \n",
    "    def add_image_caption(self, image_id: str, caption: str, image_path: str = None):\n",
    "        \"\"\"Add image caption for multimodal retrieval\"\"\"\n",
    "        self.image_captions[image_id] = {\n",
    "            'caption': caption,\n",
    "            'image_path': image_path,\n",
    "            'text_embedding': None,\n",
    "            'image_embedding': None\n",
    "        }\n",
    "    \n",
    "    def process_images_with_captions(self, image_caption_pairs: List[Tuple[str, str]]):\n",
    "        \"\"\"Process multiple image-caption pairs\"\"\"\n",
    "        print(\"Processing image captions...\")\n",
    "        \n",
    "        for image_id, caption in tqdm(image_caption_pairs, desc=\"Processing images\"):\n",
    "            self.add_image_caption(image_id, caption)\n",
    "        \n",
    "        # Generate embeddings for all captions\n",
    "        captions = [data['caption'] for data in self.image_captions.values()]\n",
    "        caption_embeddings = self.embedding_manager.encode_texts(captions)\n",
    "        \n",
    "        # Store embeddings\n",
    "        for i, (image_id, data) in enumerate(self.image_captions.items()):\n",
    "            data['text_embedding'] = caption_embeddings[i]\n",
    "        \n",
    "        print(f\"Processed {len(self.image_captions)} image captions\")\n",
    "    \n",
    "    def retrieve_multimodal(self, query: str, include_images: bool = True, \n",
    "                           top_k_text: int = 3, top_k_images: int = 2) -> Dict:\n",
    "        \"\"\"Retrieve from both text and image captions\"\"\"\n",
    "        # Get text-based retrieval\n",
    "        text_results = self.base_pipeline.retriever.retrieve(query, k=top_k_text)\n",
    "        \n",
    "        multimodal_results = {\n",
    "            'text_docs': text_results,\n",
    "            'image_docs': [],\n",
    "            'combined_context': []\n",
    "        }\n",
    "        \n",
    "        if include_images and self.image_captions:\n",
    "            # Search in image captions\n",
    "            query_embedding = self.embedding_manager.encode_texts([query])[0]\n",
    "            \n",
    "            # Calculate similarities with image captions\n",
    "            similarities = []\n",
    "            for image_id, data in self.image_captions.items():\n",
    "                if data['text_embedding'] is not None:\n",
    "                    sim = cosine_similarity(\n",
    "                        query_embedding.reshape(1, -1),\n",
    "                        data['text_embedding'].reshape(1, -1)\n",
    "                    )[0][0]\n",
    "                    similarities.append((image_id, sim, data['caption']))\n",
    "            \n",
    "            # Sort and get top-k\n",
    "            similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "            for image_id, sim_score, caption in similarities[:top_k_images]:\n",
    "                multimodal_results['image_docs'].append({\n",
    "                    'image_id': image_id,\n",
    "                    'caption': caption,\n",
    "                    'similarity': sim_score,\n",
    "                    'type': 'image_caption'\n",
    "                })\n",
    "        \n",
    "        # Combine contexts\n",
    "        text_contexts = [doc['content'] for doc in text_results]\n",
    "        image_contexts = [f\"ছবির বর্ণনা: {doc['caption']}\" for doc in multimodal_results['image_docs']]\n",
    "        \n",
    "        multimodal_results['combined_context'] = text_contexts + image_contexts\n",
    "        \n",
    "        return multimodal_results\n",
    "    \n",
    "    def ask_multimodal(self, question: str, include_images: bool = True) -> Dict:\n",
    "        \"\"\"Ask question with multimodal context\"\"\"\n",
    "        # Retrieve multimodal context\n",
    "        retrieval_results = self.retrieve_multimodal(question, include_images=include_images)\n",
    "        \n",
    "        # Generate answer using combined context\n",
    "        answer = self.base_pipeline.generator.generate_answer(\n",
    "            question, \n",
    "            retrieval_results['combined_context']\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'question': question,\n",
    "            'answer': answer,\n",
    "            'text_docs': retrieval_results['text_docs'],\n",
    "            'image_docs': retrieval_results['image_docs'],\n",
    "            'context_used': retrieval_results['combined_context']\n",
    "        }\n",
    "\n",
    "# Create sample image captions for demonstration\n",
    "sample_image_captions = [\n",
    "    (\"img001\", \"বাংলাদেশের পতাকা সবুজ রঙের উপর লাল বৃত্ত\"),\n",
    "    (\"img002\", \"ঢাকার জাতীয় সংসদ ভবন একটি আধুনিক স্থাপত্য\"),\n",
    "    (\"img003\", \"পদ্মা সেতু বাংলাদেশের দীর্ঘতম সেতু\"),\n",
    "    (\"img004\", \"রয়েল বেঙ্গল টাইগার বাংলাদেশের জাতীয় পশু\"),\n",
    "    (\"img005\", \"শাপলা বাংলাদেশের জাতীয় ফুল\")\n",
    "]\n",
    "\n",
    "# Initialize multimodal RAG\n",
    "multimodal_rag = MultimodalBongoRAG(rag_pipeline, embedding_manager)\n",
    "\n",
    "# Process sample image captions\n",
    "multimodal_rag.process_images_with_captions(sample_image_captions)\n",
    "\n",
    "# Test multimodal RAG\n",
    "print(\"\\nTesting Multimodal RAG...\")\n",
    "test_question = \"বাংলাদেশের জাতীয় প্রতীক সম্পর্কে বলুন\"\n",
    "\n",
    "multimodal_result = multimodal_rag.ask_multimodal(test_question, include_images=True)\n",
    "\n",
    "print(f\"প্রশ্ন: {multimodal_result['question']}\")\n",
    "print(f\"উত্তর: {multimodal_result['answer']}\")\n",
    "print(f\"\\nব্যবহৃত ছবি: {len(multimodal_result['image_docs'])}\")\n",
    "for img_doc in multimodal_result['image_docs']:\n",
    "    print(f\"  - {img_doc['caption']} (similarity: {img_doc['similarity']:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Fine-tuning Pipeline (LoRA Training)\n",
    "class BongoRAGTrainer:\n",
    "    \"\"\"Fine-tuning pipeline for LLaMA/Gemma models on Bangla QA data\"\"\"\n",
    "    \n",
    "    def __init__(self, config: BongoRAGConfig, generator: BongoRAGGenerator):\n",
    "        self.config = config\n",
    "        self.generator = generator\n",
    "        self.training_data = None\n",
    "        \n",
    "    def prepare_training_data(self, df: pd.DataFrame, retriever: BongoRAGRetriever) -> List[Dict]:\n",
    "        \"\"\"Prepare training data with RAG context\"\"\"\n",
    "        training_examples = []\n",
    "        \n",
    "        print(\"Preparing training data with RAG context...\")\n",
    "        \n",
    "        for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Preparing data\"):\n",
    "            question = row['Question']\n",
    "            ground_truth = row['Answer']\n",
    "            \n",
    "            # Get context from retriever\n",
    "            retrieved_docs = retriever.retrieve(question, k=3)\n",
    "            context = [doc['content'] for doc in retrieved_docs]\n",
    "            \n",
    "            # Create training prompt\n",
    "            prompt = self.generator.create_prompt(question, context)\n",
    "            \n",
    "            # Create training example\n",
    "            training_example = {\n",
    "                'input': prompt,\n",
    "                'output': ground_truth,\n",
    "                'question': question,\n",
    "                'context': context\n",
    "            }\n",
    "            training_examples.append(training_example)\n",
    "        \n",
    "        return training_examples\n",
    "    \n",
    "    def create_training_dataset(self, training_examples: List[Dict], test_size: float = 0.2):\n",
    "        \"\"\"Create train/validation split\"\"\"\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        \n",
    "        train_data, val_data = train_test_split(\n",
    "            training_examples, \n",
    "            test_size=test_size, \n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        print(f\"Training data: {len(train_data)} examples\")\n",
    "        print(f\"Validation data: {len(val_data)} examples\")\n",
    "        \n",
    "        return train_data, val_data\n",
    "    \n",
    "    def save_training_data(self, training_examples: List[Dict], filename: str = \"training_data.json\"):\n",
    "        \"\"\"Save training data to JSON file\"\"\"\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(training_examples, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"Training data saved to {filename}\")\n",
    "    \n",
    "    def load_training_data(self, filename: str = \"training_data.json\") -> List[Dict]:\n",
    "        \"\"\"Load training data from JSON file\"\"\"\n",
    "        with open(filename, 'r', encoding='utf-8') as f:\n",
    "            return json.load(f)\n",
    "\n",
    "# Demonstration of training data preparation\n",
    "trainer = BongoRAGTrainer(config, generator)\n",
    "\n",
    "# Prepare training data from a small sample\n",
    "sample_for_training = df.sample(n=100, random_state=42)  # Small sample for demonstration\n",
    "training_examples = trainer.prepare_training_data(sample_for_training, retriever)\n",
    "\n",
    "# Create train/val split\n",
    "train_data, val_data = trainer.create_training_dataset(training_examples)\n",
    "\n",
    "# Save training data\n",
    "trainer.save_training_data(training_examples, \"bongo_rag_training_data.json\")\n",
    "\n",
    "# Display sample training example\n",
    "print(\"\\nSample Training Example:\")\n",
    "print(\"=\" * 50)\n",
    "example = training_examples[0]\n",
    "print(f\"Input (Prompt):\\n{example['input'][:300]}...\")\n",
    "print(f\"\\nExpected Output:\\n{example['output']}\")\n",
    "print(\"=\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. Analysis and Visualization\n",
    "class BongoRAGAnalyzer:\n",
    "    \"\"\"Analysis and visualization tools for the RAG system\"\"\"\n",
    "    \n",
    "    def __init__(self, config: BongoRAGConfig):\n",
    "        self.config = config\n",
    "        \n",
    "    def analyze_dataset(self, df: pd.DataFrame) -> Dict:\n",
    "        \"\"\"Analyze the dataset characteristics\"\"\"\n",
    "        stats = {\n",
    "            'total_qa_pairs': len(df),\n",
    "            'avg_question_length': df['Question'].str.len().mean(),\n",
    "            'avg_answer_length': df['Answer'].str.len().mean(),\n",
    "            'unique_questions': df['Question'].nunique(),\n",
    "            'unique_answers': df['Answer'].nunique(),\n",
    "            'question_length_distribution': df['Question'].str.len().describe().to_dict(),\n",
    "            'answer_length_distribution': df['Answer'].str.len().describe().to_dict()\n",
    "        }\n",
    "        return stats\n",
    "    \n",
    "    def plot_length_distributions(self, df: pd.DataFrame):\n",
    "        \"\"\"Plot question and answer length distributions\"\"\"\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # Question length distribution\n",
    "        df['Question'].str.len().hist(bins=50, ax=ax1, alpha=0.7, color='blue')\n",
    "        ax1.set_title('Question Length Distribution')\n",
    "        ax1.set_xlabel('Number of Characters')\n",
    "        ax1.set_ylabel('Frequency')\n",
    "        ax1.axvline(df['Question'].str.len().mean(), color='red', linestyle='--', label='Mean')\n",
    "        ax1.legend()\n",
    "        \n",
    "        # Answer length distribution\n",
    "        df['Answer'].str.len().hist(bins=50, ax=ax2, alpha=0.7, color='green')\n",
    "        ax2.set_title('Answer Length Distribution')\n",
    "        ax2.set_xlabel('Number of Characters')\n",
    "        ax2.set_ylabel('Frequency')\n",
    "        ax2.axvline(df['Answer'].str.len().mean(), color='red', linestyle='--', label='Mean')\n",
    "        ax2.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def analyze_retrieval_performance(self, pipeline: BongoRAGPipeline, test_questions: List[str]):\n",
    "        \"\"\"Analyze retrieval performance\"\"\"\n",
    "        retrieval_scores = []\n",
    "        context_relevance = []\n",
    "        \n",
    "        for question in test_questions:\n",
    "            result = pipeline.ask(question)\n",
    "            retrieval_scores.extend(result['retrieval_scores'])\n",
    "            \n",
    "            # Simple relevance score based on question-context overlap\n",
    "            question_words = set(question.lower().split())\n",
    "            for context in result['context_used']:\n",
    "                context_words = set(context.lower().split())\n",
    "                overlap = len(question_words & context_words) / len(question_words) if question_words else 0\n",
    "                context_relevance.append(overlap)\n",
    "        \n",
    "        return {\n",
    "            'retrieval_scores': retrieval_scores,\n",
    "            'context_relevance': context_relevance,\n",
    "            'avg_retrieval_score': np.mean(retrieval_scores),\n",
    "            'avg_context_relevance': np.mean(context_relevance)\n",
    "        }\n",
    "    \n",
    "    def plot_retrieval_analysis(self, retrieval_stats: Dict):\n",
    "        \"\"\"Plot retrieval performance analysis\"\"\"\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # Retrieval scores distribution\n",
    "        ax1.hist(retrieval_stats['retrieval_scores'], bins=30, alpha=0.7, color='purple')\n",
    "        ax1.set_title('Retrieval Scores Distribution')\n",
    "        ax1.set_xlabel('FAISS Distance Score')\n",
    "        ax1.set_ylabel('Frequency')\n",
    "        ax1.axvline(retrieval_stats['avg_retrieval_score'], color='red', linestyle='--', label='Mean')\n",
    "        ax1.legend()\n",
    "        \n",
    "        # Context relevance distribution\n",
    "        ax2.hist(retrieval_stats['context_relevance'], bins=30, alpha=0.7, color='orange')\n",
    "        ax2.set_title('Context Relevance Distribution')\n",
    "        ax2.set_xlabel('Word Overlap Ratio')\n",
    "        ax2.set_ylabel('Frequency')\n",
    "        ax2.axvline(retrieval_stats['avg_context_relevance'], color='red', linestyle='--', label='Mean')\n",
    "        ax2.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def generate_performance_report(self, metrics: Dict, dataset_stats: Dict) -> str:\n",
    "        \"\"\"Generate comprehensive performance report\"\"\"\n",
    "        report = f\"\"\"\n",
    "# BongoRAG Performance Report\n",
    "\n",
    "## Dataset Statistics\n",
    "- Total QA Pairs: {dataset_stats['total_qa_pairs']:,}\n",
    "- Average Question Length: {dataset_stats['avg_question_length']:.1f} characters\n",
    "- Average Answer Length: {dataset_stats['avg_answer_length']:.1f} characters\n",
    "- Unique Questions: {dataset_stats['unique_questions']:,}\n",
    "- Unique Answers: {dataset_stats['unique_answers']:,}\n",
    "\n",
    "## Model Performance\n",
    "### Text Generation Metrics\n",
    "- Exact Match: {metrics['avg_exact_match']:.4f} ± {metrics['std_exact_match']:.4f}\n",
    "- F1 Score: {metrics['avg_f1_score']:.4f} ± {metrics['std_f1_score']:.4f}\n",
    "- BLEU Score: {metrics['avg_bleu']:.4f} ± {metrics['std_bleu']:.4f}\n",
    "\n",
    "### ROUGE Scores\n",
    "- ROUGE-1: {metrics['avg_rouge1']:.4f} ± {metrics['std_rouge1']:.4f}\n",
    "- ROUGE-2: {metrics['avg_rouge2']:.4f} ± {metrics['std_rouge2']:.4f}\n",
    "- ROUGE-L: {metrics['avg_rougeL']:.4f} ± {metrics['std_rougeL']:.4f}\n",
    "\n",
    "### Retrieval Performance\n",
    "- Precision@K: {metrics['avg_precision_at_k']:.4f} ± {metrics['std_precision_at_k']:.4f}\n",
    "- MRR: {metrics['avg_mrr']:.4f} ± {metrics['std_mrr']:.4f}\n",
    "- Average Retrieval Score: {metrics['avg_retrieval_scores']:.4f} ± {metrics['std_retrieval_scores']:.4f}\n",
    "\n",
    "## Recommendations\n",
    "1. Consider increasing the training data size for better performance\n",
    "2. Experiment with different retrieval strategies (e.g., hybrid search)\n",
    "3. Fine-tune the embedding model on domain-specific Bangla text\n",
    "4. Implement more sophisticated context ranking algorithms\n",
    "5. Add more image captions for better multimodal performance\n",
    "        \"\"\"\n",
    "        return report\n",
    "\n",
    "# Initialize analyzer\n",
    "analyzer = BongoRAGAnalyzer(config)\n",
    "\n",
    "# Analyze dataset\n",
    "dataset_stats = analyzer.analyze_dataset(df)\n",
    "print(\"Dataset Analysis:\")\n",
    "for key, value in dataset_stats.items():\n",
    "    if isinstance(value, dict):\n",
    "        print(f\"{key}:\")\n",
    "        for k, v in value.items():\n",
    "            print(f\"  {k}: {v:.2f}\" if isinstance(v, float) else f\"  {k}: {v}\")\n",
    "    else:\n",
    "        print(f\"{key}: {value:.2f}\" if isinstance(value, float) else f\"{key}: {value}\")\n",
    "\n",
    "# Plot distributions\n",
    "analyzer.plot_length_distributions(df)\n",
    "\n",
    "# Analyze retrieval performance\n",
    "test_questions_for_analysis = test_questions[:5]  # Use first 5 test questions\n",
    "retrieval_stats = analyzer.analyze_retrieval_performance(rag_pipeline, test_questions_for_analysis)\n",
    "print(f\"\\nRetrieval Analysis:\")\n",
    "print(f\"Average Retrieval Score: {retrieval_stats['avg_retrieval_score']:.4f}\")\n",
    "print(f\"Average Context Relevance: {retrieval_stats['avg_context_relevance']:.4f}\")\n",
    "\n",
    "# Plot retrieval analysis\n",
    "analyzer.plot_retrieval_analysis(retrieval_stats)\n",
    "\n",
    "# Generate comprehensive report\n",
    "report = analyzer.generate_performance_report(metrics, dataset_stats)\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. Production Utilities and Deployment Helpers\n",
    "class BongoRAGProduction:\n",
    "    \"\"\"Production utilities for the BongoRAG system\"\"\"\n",
    "    \n",
    "    def __init__(self, config: BongoRAGConfig):\n",
    "        self.config = config\n",
    "        \n",
    "    def create_inference_pipeline(self, \n",
    "                                  vector_store_path: str = None,\n",
    "                                  model_type: str = \"llama\") -> BongoRAGPipeline:\n",
    "        \"\"\"Create a production-ready inference pipeline\"\"\"\n",
    "        \n",
    "        # Load embedding manager\n",
    "        embedding_manager = BanglaEmbeddingManager(self.config)\n",
    "        \n",
    "        # Load retriever and vector store\n",
    "        retriever = BongoRAGRetriever(self.config, embedding_manager)\n",
    "        \n",
    "        if vector_store_path:\n",
    "            success = retriever.load_vector_store(vector_store_path)\n",
    "            if not success:\n",
    "                raise ValueError(f\"Failed to load vector store from {vector_store_path}\")\n",
    "        else:\n",
    "            retriever.load_vector_store()  # Load from default path\n",
    "        \n",
    "        # Load generator (use mock for demonstration)\n",
    "        generator = MockBongoRAGGenerator(self.config)\n",
    "        generator.load_model(model_type)\n",
    "        generator.setup_lora()\n",
    "        \n",
    "        # Create pipeline\n",
    "        pipeline = BongoRAGPipeline(retriever, generator, self.config)\n",
    "        \n",
    "        return pipeline\n",
    "    \n",
    "    def batch_inference(self, \n",
    "                       pipeline: BongoRAGPipeline, \n",
    "                       questions: List[str],\n",
    "                       output_file: str = \"batch_results.json\") -> List[Dict]:\n",
    "        \"\"\"Run batch inference and save results\"\"\"\n",
    "        \n",
    "        print(f\"Running batch inference on {len(questions)} questions...\")\n",
    "        results = pipeline.batch_ask(questions)\n",
    "        \n",
    "        # Save results\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        print(f\"Results saved to {output_file}\")\n",
    "        return results\n",
    "    \n",
    "    def create_api_response_format(self, result: Dict) -> Dict:\n",
    "        \"\"\"Format RAG result for API response\"\"\"\n",
    "        return {\n",
    "            'question': result['question'],\n",
    "            'answer': result['answer'],\n",
    "            'confidence': 1.0 - min(result['retrieval_scores']) if result['retrieval_scores'] else 0.0,\n",
    "            'sources': [\n",
    "                {\n",
    "                    'content': doc['content'][:200] + '...' if len(doc['content']) > 200 else doc['content'],\n",
    "                    'score': doc['score'],\n",
    "                    'rank': doc['rank']\n",
    "                }\n",
    "                for doc in result['retrieved_docs'][:3]  # Top 3 sources\n",
    "            ],\n",
    "            'metadata': {\n",
    "                'retrieval_method': 'faiss_semantic_search',\n",
    "                'embedding_model': self.config.BANGLA_EMBEDDING_MODEL,\n",
    "                'generation_model': 'mock_llama',  # In production, use actual model name\n",
    "                'top_k_retrieval': len(result['retrieved_docs'])\n",
    "            }\n",
    "        }\n",
    "\n",
    "# Create production utilities\n",
    "production = BongoRAGProduction(config)\n",
    "\n",
    "# Demonstrate production pipeline creation\n",
    "print(\"Creating production-ready inference pipeline...\")\n",
    "inference_pipeline = production.create_inference_pipeline()\n",
    "\n",
    "# Test production pipeline\n",
    "test_production_questions = [\n",
    "    \"বাংলাদেশের স্বাধীনতা কবে হয়েছিল?\",\n",
    "    \"ঢাকা কোন নদীর তীরে অবস্থিত?\"\n",
    "]\n",
    "\n",
    "production_results = production.batch_inference(\n",
    "    inference_pipeline, \n",
    "    test_production_questions,\n",
    "    \"production_test_results.json\"\n",
    ")\n",
    "\n",
    "# Format results for API\n",
    "print(\"\\nAPI Response Format Examples:\")\n",
    "for result in production_results[:2]:\n",
    "    api_response = production.create_api_response_format(result)\n",
    "    print(json.dumps(api_response, ensure_ascii=False, indent=2)[:500] + \"...\")\n",
    "    print(\"---\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 13. Installation Instructions and Dependencies\n",
    "\n",
    "To run this BongoRAG implementation, you'll need to install the following dependencies. Save this as a `requirements.txt` file and run `pip install -r requirements.txt`.\n",
    "\n",
    "### Required Packages:\n",
    "```\n",
    "# Core ML packages\n",
    "torch>=2.0.0\n",
    "transformers>=4.30.0\n",
    "sentence-transformers>=2.2.0\n",
    "accelerate>=0.20.0\n",
    "\n",
    "# RAG and Vector Store\n",
    "langchain>=0.0.200\n",
    "faiss-cpu>=1.7.0\n",
    "# For GPU: faiss-gpu>=1.7.0\n",
    "\n",
    "# Fine-tuning\n",
    "peft>=0.4.0\n",
    "bitsandbytes>=0.41.0\n",
    "\n",
    "# Evaluation\n",
    "rouge-score>=0.1.2\n",
    "sacrebleu>=2.3.0\n",
    "scikit-learn>=1.3.0\n",
    "\n",
    "# Data processing\n",
    "pandas>=2.0.0\n",
    "numpy>=1.24.0\n",
    "\n",
    "# Visualization\n",
    "matplotlib>=3.7.0\n",
    "seaborn>=0.12.0\n",
    "tqdm>=4.65.0\n",
    "\n",
    "# Optional: for actual model access\n",
    "# huggingface_hub>=0.15.0\n",
    "```\n",
    "\n",
    "### Installation Steps:\n",
    "\n",
    "1. **Create virtual environment:**\n",
    "   ```bash\n",
    "   python -m venv bongo_rag_env\n",
    "   source bongo_rag_env/bin/activate  # On Windows: bongo_rag_env\\Scripts\\activate\n",
    "   ```\n",
    "\n",
    "2. **Install PyTorch (check your CUDA version):**\n",
    "   ```bash\n",
    "   # For CPU only\n",
    "   pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
    "   \n",
    "   # For CUDA 11.8\n",
    "   pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "   \n",
    "   # For CUDA 12.1\n",
    "   pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "   ```\n",
    "\n",
    "3. **Install other dependencies:**\n",
    "   ```bash\n",
    "   pip install -r requirements.txt\n",
    "   ```\n",
    "\n",
    "4. **For actual LLaMA/Gemma models (requires HuggingFace access):**\n",
    "   ```bash\n",
    "   # Login to HuggingFace\n",
    "   huggingface-cli login\n",
    "   \n",
    "   # Request access to LLaMA-2 and Gemma models through HuggingFace\n",
    "   ```\n",
    "\n",
    "### Hardware Requirements:\n",
    "- **Minimum:** 16GB RAM, modern CPU\n",
    "- **Recommended:** 32GB+ RAM, GPU with 12GB+ VRAM for full model inference\n",
    "- **Storage:** 50GB+ for models and vector stores\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 14. Usage Guide and Next Steps\n",
    "\n",
    "### Quick Start Guide:\n",
    "\n",
    "1. **Basic RAG Query:**\n",
    "   ```python\n",
    "   # Ask a question\n",
    "   result = rag_pipeline.ask(\"বাংলাদেশের রাজধানী কি?\")\n",
    "   print(f\"উত্তর: {result['answer']}\")\n",
    "   ```\n",
    "\n",
    "2. **Multimodal Query:**\n",
    "   ```python\n",
    "   # Ask with image context\n",
    "   multimodal_result = multimodal_rag.ask_multimodal(\"বাংলাদেশের জাতীয় প্রতীক সম্পর্কে বলুন\")\n",
    "   print(f\"উত্তর: {multimodal_result['answer']}\")\n",
    "   ```\n",
    "\n",
    "3. **Batch Processing:**\n",
    "   ```python\n",
    "   questions = [\"প্রশ্ন ১\", \"প্রশ্ন ২\", \"প্রশ্ন ৩\"]\n",
    "   results = rag_pipeline.batch_ask(questions)\n",
    "   ```\n",
    "\n",
    "4. **Evaluation:**\n",
    "   ```python\n",
    "   test_df = df.sample(n=100)\n",
    "   metrics = evaluator.evaluate_pipeline(rag_pipeline, test_df)\n",
    "   evaluator.print_evaluation_report(metrics)\n",
    "   ```\n",
    "\n",
    "### For Production Deployment:\n",
    "\n",
    "1. **Replace Mock Generator with Real Models:**\n",
    "   - Obtain access to LLaMA-2 or Gemma models\n",
    "   - Replace `MockBongoRAGGenerator` with `BongoRAGGenerator`\n",
    "   - Load actual models using the provided configuration\n",
    "\n",
    "2. **Scale Vector Store:**\n",
    "   - For larger datasets, consider using distributed FAISS\n",
    "   - Implement vector store sharding for better performance\n",
    "\n",
    "3. **Fine-tune Models:**\n",
    "   - Use the `BongoRAGTrainer` class to fine-tune models\n",
    "   - Implement actual training loops with your preferred framework\n",
    "\n",
    "4. **Deploy as API:**\n",
    "   - Create Flask/FastAPI endpoints using the production utilities\n",
    "   - Implement caching and load balancing\n",
    "\n",
    "### Research Extensions:\n",
    "\n",
    "1. **Improve Retrieval:**\n",
    "   - Implement hybrid search (semantic + keyword)\n",
    "   - Add reranking models\n",
    "   - Experiment with different embedding models\n",
    "\n",
    "2. **Enhance Generation:**\n",
    "   - Fine-tune models on domain-specific data\n",
    "   - Implement chain-of-thought prompting\n",
    "   - Add controllable generation parameters\n",
    "\n",
    "3. **Multimodal Improvements:**\n",
    "   - Add support for actual images (not just captions)\n",
    "   - Implement vision-language models\n",
    "   - Create image-text alignment metrics\n",
    "\n",
    "4. **Evaluation:**\n",
    "   - Add human evaluation framework\n",
    "   - Implement domain-specific metrics\n",
    "   - Create comparative benchmarks\n",
    "\n",
    "### Files Created:\n",
    "- `faiss_index/`: Vector store files\n",
    "- `models/`: Cached model files  \n",
    "- `bongo_rag_training_data.json`: Training data\n",
    "- `production_test_results.json`: Production test results\n",
    "\n",
    "This implementation provides a complete foundation for your thesis research on Multimodal RAG for Bangla Question Answering!\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
